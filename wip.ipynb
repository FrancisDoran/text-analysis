{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\p0pp1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\p0pp1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re # text parsing and clean up\n",
    "import nltk # data resources and cleaning tools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd # tbd tabular data manipulation\n",
    "from collections import Counter # counting word frequency\n",
    "import os # interacting with directories and file names\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "brown_corpus = brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = os.listdir('texts/') \n",
    "# gets all files and subdirectories in the text directory,\n",
    "#if there were subdirectories or files that can't be interperted as str we would error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "words = []\n",
    "punctuation_edges = re.compile(r'[^\\w\\s]\\B|\\B[^\\w\\s]\\b')\n",
    "inbetween = re.compile(r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK[\\S\\s]*?\\*\\*\\* S.*')\n",
    "start_marker = re.compile(r'The Project Gutenberg eBook of[\\S\\s]*?\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK.*')\n",
    "end_marker = re.compile(r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK[\\s\\S]*?eBooks\\.')\n",
    "all_text = ''\n",
    "for txt in txts: # loop through all files in our texts directory\n",
    "    with open(f'texts/{txt}','r', encoding='utf-8') as file:\n",
    "        # clean the data\n",
    "        all_text += ' ' + file.read()\n",
    "all_text = inbetween.sub('', all_text)\n",
    "all_text = start_marker.sub('', all_text, count=1) # finds and removes the meta data from the text\n",
    "all_text = end_marker.sub('', all_text, count=1)\n",
    "all_text = all_text.lower() # all lowercase for uniformity and to count Bee and bee as the same word.\n",
    "all_text = punctuation_edges.sub('', all_text) # Removes punctuation on the edges of a word\n",
    "words = all_text.split() # makes the string into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [w for w in words if w not in stop_words and len(w)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_texts_length = len(filtered_words)\n",
    "selected_texts_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_texts_counter = Counter(filtered_words)\n",
    "selected_texts_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus_joined = ' '.join(brown_corpus)\n",
    "brown_corpus_joined = brown_corpus_joined.lower()\n",
    "brown_corpus = brown_corpus_joined.split()\n",
    "filtered_brown_corpus = [word for word in brown_corpus if word not in stop_words and len(word)>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528829"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_counter = Counter(filtered_brown_corpus)\n",
    "brown_length = len(filtered_brown_corpus)\n",
    "brown_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_df = pd.DataFrame({'word':brown_counter.keys(),'count':brown_counter.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_texts_df = pd.DataFrame({'word':selected_texts_counter.keys(),'count':selected_texts_counter.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fulton</td>\n",
       "      <td>17</td>\n",
       "      <td>0.003215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>county</td>\n",
       "      <td>155</td>\n",
       "      <td>0.029310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grand</td>\n",
       "      <td>48</td>\n",
       "      <td>0.009077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jury</td>\n",
       "      <td>67</td>\n",
       "      <td>0.012670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>said</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.370819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49330</th>\n",
       "      <td>aviary</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49331</th>\n",
       "      <td>olive-flushed</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49332</th>\n",
       "      <td>coral-colored</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49333</th>\n",
       "      <td>boucle</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49334</th>\n",
       "      <td>stupefying</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49335 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  count   percent\n",
       "0             fulton     17  0.003215\n",
       "1             county    155  0.029310\n",
       "2              grand     48  0.009077\n",
       "3               jury     67  0.012670\n",
       "4               said   1961  0.370819\n",
       "...              ...    ...       ...\n",
       "49330         aviary      1  0.000189\n",
       "49331  olive-flushed      1  0.000189\n",
       "49332  coral-colored      1  0.000189\n",
       "49333         boucle      1  0.000189\n",
       "49334     stupefying      1  0.000189\n",
       "\n",
       "[49335 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_df['percent'] = brown_df['count']/brown_length * 100\n",
    "brown_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(brown_corpus_joined[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = np.unique(tokens)\n",
    "unique_token_index = {token: idx for idx, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 10\n",
    "input_words = []\n",
    "next_words = []\n",
    "for i in range(len(tokens)-n_words):\n",
    "    input_words.append(tokens[i:i+n_words])\n",
    "    next_words.append(tokens[i+n_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_words), n_words, len(unique_tokens)), dtype=bool)\n",
    "y = np.zeros((len(next_words), len(unique_tokens)), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, words in enumerate(input_words):\n",
    "    for j, word in enumerate(words):\n",
    "        X[i, j, unique_token_index[word]] = 1\n",
    "    y[i, unique_token_index[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(unique_tokens)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 14s 88ms/step - loss: 6.8425 - accuracy: 0.0743\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 11s 89ms/step - loss: 6.6089 - accuracy: 0.0821\n",
      "Epoch 3/30\n",
      "128/128 [==============================] - 11s 88ms/step - loss: 6.4551 - accuracy: 0.0892\n",
      "Epoch 4/30\n",
      "128/128 [==============================] - 11s 89ms/step - loss: 6.2788 - accuracy: 0.0941\n",
      "Epoch 5/30\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 6.0465 - accuracy: 0.1080\n",
      "Epoch 6/30\n",
      "128/128 [==============================] - 11s 88ms/step - loss: 5.7995 - accuracy: 0.1229\n",
      "Epoch 7/30\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 5.5376 - accuracy: 0.1395\n",
      "Epoch 8/30\n",
      "128/128 [==============================] - 11s 86ms/step - loss: 5.2594 - accuracy: 0.1589\n",
      "Epoch 9/30\n",
      "128/128 [==============================] - 11s 88ms/step - loss: 4.9794 - accuracy: 0.1806\n",
      "Epoch 10/30\n",
      "128/128 [==============================] - 11s 89ms/step - loss: 4.7018 - accuracy: 0.2077\n",
      "Epoch 11/30\n",
      "128/128 [==============================] - 12s 90ms/step - loss: 4.4058 - accuracy: 0.2376\n",
      "Epoch 12/30\n",
      "128/128 [==============================] - 11s 88ms/step - loss: 4.1108 - accuracy: 0.2693\n",
      "Epoch 13/30\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 3.8160 - accuracy: 0.2996\n",
      "Epoch 14/30\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 3.5264 - accuracy: 0.3417\n",
      "Epoch 15/30\n",
      "128/128 [==============================] - 16s 123ms/step - loss: 3.2314 - accuracy: 0.3852\n",
      "Epoch 16/30\n",
      "128/128 [==============================] - 13s 101ms/step - loss: 2.9567 - accuracy: 0.4284\n",
      "Epoch 17/30\n",
      "128/128 [==============================] - 13s 98ms/step - loss: 2.6944 - accuracy: 0.4715\n",
      "Epoch 18/30\n",
      "128/128 [==============================] - 13s 105ms/step - loss: 2.4330 - accuracy: 0.5197\n",
      "Epoch 19/30\n",
      "128/128 [==============================] - 13s 105ms/step - loss: 2.2043 - accuracy: 0.5674\n",
      "Epoch 20/30\n",
      "128/128 [==============================] - 13s 98ms/step - loss: 1.9725 - accuracy: 0.6168\n",
      "Epoch 21/30\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.7513 - accuracy: 0.6728\n",
      "Epoch 22/30\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 1.5600 - accuracy: 0.7212\n",
      "Epoch 23/30\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.3703 - accuracy: 0.7683\n",
      "Epoch 24/30\n",
      "128/128 [==============================] - 12s 90ms/step - loss: 1.2119 - accuracy: 0.8043\n",
      "Epoch 25/30\n",
      "128/128 [==============================] - 12s 90ms/step - loss: 1.0608 - accuracy: 0.8386\n",
      "Epoch 26/30\n",
      "128/128 [==============================] - 11s 89ms/step - loss: 0.9175 - accuracy: 0.8741\n",
      "Epoch 27/30\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7979 - accuracy: 0.8983\n",
      "Epoch 28/30\n",
      "128/128 [==============================] - 13s 100ms/step - loss: 0.6835 - accuracy: 0.9198\n",
      "Epoch 29/30\n",
      "128/128 [==============================] - 13s 99ms/step - loss: 0.5867 - accuracy: 0.9361\n",
      "Epoch 30/30\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.4767 - accuracy: 0.9539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d26a5c9a50>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=RMSprop(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "model.fit(X, y, batch_size=128, epochs=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p0pp1\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, n_best):\n",
    "    input_text = input_text.lower()\n",
    "    X = np.zeros((1, n_words, len(unique_tokens)))\n",
    "    for i, word in enumerate(input_text.split()):\n",
    "        X[0, i, unique_token_index[word]] = 1\n",
    "    predictions = model.predict(X)[0]\n",
    "    return np.argpartition(predictions, n_best)[-n_best:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'life,'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m possible \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_next_word\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe details of john early life, as frankly set down in \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUp from Slavery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[72], line 5\u001b[0m, in \u001b[0;36mpredict_next_word\u001b[1;34m(input_text, n_best)\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, n_words, \u001b[38;5;28mlen\u001b[39m(unique_tokens)))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_text\u001b[38;5;241m.\u001b[39msplit()):\n\u001b[1;32m----> 5\u001b[0m     X[\u001b[38;5;241m0\u001b[39m, i, \u001b[43munique_token_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margpartition(predictions, n_best)[\u001b[38;5;241m-\u001b[39mn_best:]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'life,'"
     ]
    }
   ],
   "source": [
    "possible = predict_next_word(\"The details of john early life, as frankly set down in 'Up from Slavery'\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1920', '15', 'youth', '13th', 'karns']\n"
     ]
    }
   ],
   "source": [
    "print([unique_tokens[idx] for idx in possible])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
